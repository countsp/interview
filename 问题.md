
遇到什么问题，如何解决。

batch_size

warm-up

多卡训练dp-ddp

梯度爆炸/消失

学习率大



cnn/resnet/yolo

# Transformer

### 1. Q 和 K 的维度要求
- **Q 和 K 的最后一维（特征维度 \(d_k\)）必须相同**  
  因为注意力权重计算是：
  $$
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
  $$
  矩阵乘法要求：
  $$
  Q \in \mathbb{R}^{n_q \times d_k},\quad K \in \mathbb{R}^{n_k \times d_k}
  $$
  这样才能做点积。  
- 其中：
  - \(n_q\) = Query 的 token 数（输出 token 数）
  - \(n_k\) = Key/Value 的 token 数（信息源 token 数）

---

### 2. V 的维度要求
- **V 的第一维必须与 K 的第一维相同**  
  因为注意力权重矩阵形状是：
  $$
  \text{Attention Weights} \in \mathbb{R}^{n_q \times n_k}
  $$
  它会乘在：
  $$
  V \in \mathbb{R}^{n_k \times d_v}
  $$
  上，因此 \(n_k\) 必须一致。  
- \(d_v\) 可以不同于 \(d_k\)，例如注意力计算用低维，输出特征需要高维。

---

### 3. 总结维度关系
$$
Q \in \mathbb{R}^{n_q \times d_k}, \quad
K \in \mathbb{R}^{n_k \times d_k}, \quad
V \in \mathbb{R}^{n_k \times d_v}
$$

- \(n_q\)：Q 的 token 数，决定输出长度  
- \(n_k\)：K/V 的 token 数，信息源数量  
- \(d_k\)：Q、K 的特征维度（点积计算空间）  
- \(d_v\)：V 的特征维度（输出特征维度）

---

### 总结
> Q 和 K 的特征维度必须一致；K 和 V 的 token 数必须一致；V 的特征维度可与 Q/K 不同。

### 反向传播
在训练神经网络时，优化器（SGD、Adam等）会根据损失函数对参数的梯度来更新参数：

W -> W-η * ∂L/∂W

​
<img width="833" height="290" alt="Screenshot from 2025-08-13 09-27-41" src="https://github.com/user-attachments/assets/42e1b95a-5d7f-47f7-a7db-286ebe2e4cfd" />


### sigmoid & softmax

**sigmoid**

<img width="858" height="251" alt="Screenshot from 2025-08-13 08-50-13" src="https://github.com/user-attachments/assets/9c46f16f-5f1e-4c16-8faa-e951941dedac" />

**softmax**

<img width="860" height="462" alt="Screenshot from 2025-08-13 08-51-02" src="https://github.com/user-attachments/assets/b52d921f-2fe3-47ba-a14e-08ec2ddd5de7" />

### BN Batch Normalization
<img width="869" height="446" alt="Screenshot from 2025-08-13 09-31-08" src="https://github.com/user-attachments/assets/4158a6e0-7753-4469-a19a-ea6bf221a6af" />

这里的 ε（epsilon） 是一个很小的常数，用来防止分母为零

### 训练和测试的区别

**1. 数据使用**

训练阶段：使用训练集数据，数据通常会经过数据增强（例如随机裁剪、翻转、颜色抖动等），以增加样本多样性。

测试阶段：使用测试集或验证集数据，一般不做随机数据增强，只做必要的归一化或尺寸调整，保证结果稳定、可重复。

**2. 模型参数更新**

训练阶段：需要进行前向传播、反向传播，并根据损失函数计算梯度，通过优化器（SGD、Adam 等）更新模型参数。

测试阶段：只进行前向传播，不计算梯度，也不更新参数（通常会关闭 requires_grad 或使用 torch.no_grad()）。

**3. 模型内部算子的行为**

1.Batch Normalization (BN)

训练时使用当前 mini-batch 的均值和方差来归一化，并更新全局的移动平均均值/方差。

测试时使用训练过程中累积的全局均值和方差，不依赖当前批次数据。

2.Dropout

训练时按设定比例随机丢弃神经元（屏蔽部分特征），防止过拟合。

测试时关闭 Dropout，使用全部神经元。

---

**1. 问：请解释 BEV（Bird’s Eye View）在自动驾驶感知中的作用、实现方式，以及 BEVFormer 相比传统 BEV 算法的改进点。**

**答：**

- **作用**：
  - BEV 将多视角图像（或融合雷达点云）**投影到统一的俯视平面坐标系**，**消除透视畸变**，便于检测、分割、跟踪。
  - 在自动驾驶中，**规划与决策模块更容易基于 BEV 数据进行路径计算**。
- **传统 BEV 实现**：
  1. **Inverse Perspective Mapping (IPM)**：通过相机内外参将图像像素反投影到地平面，**速度快但对高度信息敏感**。
  2. **Lift-Splat-Shoot (LSS)**：先预测每个像素的深度概率分布，再将特征升维到 3D 空间投影到 BEV 平面，精度更高。
- **BEVFormer 改进**：
  1. 使用 **Transformer** 结构，将 BEV Query 作为可学习参数，进行跨视角采样（Spatial Cross-Attention）。
  2. 引入 **Temporal Self-Attention** 融合历史帧信息，提升动态目标检测和遮挡恢复能力。
  3. 多尺度位置编码增强远距离物体的检测效果。


<img width="597" height="1121" alt="Screenshot from 2025-08-11 13-50-41" src="https://github.com/user-attachments/assets/9f4ffae7-7418-4b17-83f9-dbaef8a3caeb" />


------

**2. 问：YOLO 系列、CenterNet 和 DETR 在自动驾驶中的优缺点对比，如果要部署到高通 8155 或地平线 J5，你会选择哪种？**

**答：**

- **YOLO 系列（YOLOv5/YOLOv8）**

  - 优点：推理速度快、轻量化版本可适应低算力、生态成熟。

  - 缺点：Anchor 机制对小目标敏感（YOLOv8 已 Anchor-free 改进）。

    anchor 机制是：在特征图每个位置预设几种固定比例的参考框，让网络预测相对于这些参考框的偏移量（位置+尺度），这样检测任务更稳定，收敛更快。

- **CenterNet**

  - 优点：Anchor-free，基于关键点回归中心点和尺寸，适合密集小目标。

  - 缺点：对 backbone 依赖较大，速度可能不如 YOLO。

    整体分为 **Backbone + Head** 两部分：

    ### **(1) Backbone**

    - 用常见的卷积网络提取特征（如 ResNet、DLA、Hourglass）
    - 输出较低分辨率的特征图（stride = 4 或 8）

    ### **(2) Detection Head**

    Head 通常分成三个分支（在同一个特征图上并行预测）：

    1. **Heatmap 分支**

       - 输出形状：(C,H,W) 

         **eg:** 

         heatmap[:, 60, 90] = [0.01, 0.85, 0.02]

         位置 (60, 90) 是特征图上的一个 cell（对应原图的 (240, 360) 附近）

         在这个位置，类别 1（人行道）的概率 0.85 最大，模型认为这里是人行道目标中心点

       - C 是类别数，每个像素位置的值表示该位置是某类目标中心点的概率

       - 训练时用高斯核在中心点位置生成 label

    2. **尺寸分支（wh）**

       - 输出形状：(2,H,W)

       - 表示该位置预测的目标宽、高

         wh[:, 60, 90] = [40.5, 80.2]

         预测目标宽度 = 40.5 像素，高度 = 80.2 像素（这里宽高是在原图尺度上的值）

    3. **偏移分支（offset）**

       - 输出形状：(2,H,W)

       - 因为特征图下采样，中心点可能落在 cell 内部，需要预测精确偏移量

         offset[:, 60, 90] = [0.32, 0.45]

         

         grid_x=90，offsetx=0.32→ xcenter=(90+0.32)×4=361.28

         grid_y=60，offsety=0.45 → ycenter=(60+0.45)×4=241.8

         最终检测框：

         ```
         中心点 = (361.28, 241.8)
         宽 = 40.5，高 = 80.2
         ```

- **DETR**

  - 优点：端到端检测，无需 NMS，建模全局关系好。
  - 缺点：收敛慢，计算量大，车端部署难度高。

- **部署建议**：

  - 高通/地平线算力有限 → **YOLOv8n/s + INT8 量化**
  - 高算力 + 多任务（检测+分割+跟踪） → **轻量化 Deformable DETR**

### loss

**Focal Loss**

<img width="888" height="290" alt="Screenshot from 2025-08-13 08-39-33" src="https://github.com/user-attachments/assets/6acdb930-13c8-4284-88b2-4d3edf34b993" />

**交叉熵**

<img width="854" height="274" alt="Screenshot from 2025-08-13 08-46-20" src="https://github.com/user-attachments/assets/4a4a8b8d-1edd-47de-b785-85e1ec9a9b8d" />




------

**3. 问：描述一个多传感器融合（相机+毫米波雷达）障碍物检测系统的典型流程。**

**答：**

1. **时间同步**：硬件触发或软件时间戳对齐。
2. **空间标定**：标定外参，将雷达数据转换到相机坐标系或世界坐标系。
3. **融合方式**：
   - 早期融合：雷达点映射到图像平面作为附加输入通道。
   - 中期融合：提取特征后在 BEV 空间融合（concat / add）。
   - 晚期融合：独立检测结果在统一坐标系下融合（卡尔曼滤波 / 多目标跟踪）。
4. **输出**：融合后的检测结果，提升雨雾等极端环境下的稳定性。

------

**4. 问：自动驾驶目标检测中，如何提升远距离小目标检测效果？**

**答：**

- **数据层**：增加远距离样本采集/合成，长焦相机布局。
- **模型层**：高分辨率输入（≥1280x1280）、增强 FPN（PAFPN/HRNet）、小目标优化 anchor 或 Anchor-free。
- **训练层**：调整 Loss（Focal Loss）、在线难例挖掘（OHEM）。

------

**5. 问：立体视觉三维重建的基本原理是什么？在车端如何优化实时性？**

**答：**

- **原理**：通过匹配双目图像的同名点计算视差，利用 Z=f⋅BdZ = \frac{f \cdot B}{d}Z=df⋅B 求深度。
- **优化实时性**：
  - 算法优化：SGM、PatchMatch Stereo、稀疏匹配+深度补全。
  - 工程优化：CUDA 加速代价聚合，降低分辨率，ROI 匹配，滑动窗口复用计算结果。

------

**6. 问：在分割任务中，如何提升对边界和细长目标的分割效果？**

**答：**

- 高分辨率特征（HRNet）；
- 边界感知 Loss（Boundary Loss / Dice+CE）；
- 多尺度上下文模块（ASPP、PPM）；
- 后处理（CRF、Contour Refinement）细化边缘。

------

**7. 问：BEV 模型中如何利用时序信息提升动态目标检测？**

**答：**

- Temporal Self-Attention 融合历史帧；
- 光流引导特征对齐；
- 多帧输入的 3D 卷积 / RNN；
- 保证时序一致性，减少 Ghost 现象。

------

**8. 问：车位检测常见方法和难点是什么？**

**答：**

- **方法**：Hough 直线检测、深度学习关键点回归（LaneNet、KeypointNet）。
- **难点**：光照变化、阴影、标线磨损、被车遮挡。

------

## **二、工程实现与调优（权重 30%）**

------

**9. 问：高通/地平线车端部署感知模型常见优化手段有哪些？**

**答：**

- 模型结构轻量化（MobileNet、PP-LCNet、RepVGG）；
- INT8 量化（QAT/PTQ）；
- SDK 高效算子；
- 分辨率裁剪；
- 多线程流水线；
- 零拷贝内存管理。

------

**10. 问：模型在高速 NOA 场景漏检率升高，如何排查？**

**答：**

1. 分析漏检样本（远距/夜间/逆光）；
2. 检查标注准确性；
3. 数据增强与补充；
4. Loss 调整（Focal Loss）；
5. 引入时序信息（多帧融合）。

------

**11. 问：感知算法 badcase 分析闭环的流程是什么？**

**答：**

- 数据采集 → 标注 → 训练 → 验证 → 部署 → badcase 收集 → 回归测试 → 模型迭代。
- 辅助工具：可视化平台、数据管理系统（DVC）。

------

**12. 问：TensorRT 部署中常见性能瓶颈及优化办法？**

**答：**

- 算子不支持 → 自定义插件；
- 显存占用大 → 减少 batch / 分辨率；
- 内存拷贝多 → 零拷贝；
- 精度损失 → 混合精度或分层量化。

------

**13. 问：如何在保证精度的同时优化实时性？**

**答：**

- 剪枝 + 量化联合；
- 模型蒸馏；
- 分辨率自适应；
- 异步推理与流水线处理。

------

**14. 问：如何用 OpenCV 实现视频流实时推理并可视化检测结果？**

**答：**

- `cv::VideoCapture` 读取；
- `cv::rectangle` 绘框，`cv::putText` 显示标签；
- 帧率优化：批量推理 / GPU 加速。

------

**15. 问：自动驾驶中摄像头标定的重要性及方法？**

**答：**

- **重要性**：保证感知结果与真实物理位置对齐。
- **方法**：棋盘格标定内参；PnP 解法外参；激光辅助标定提升精度。

------

## **三、编程与数据处理（权重 20%）**

------

**16. 问：C++ 实现 Sobel 边缘检测并显示结果。**

**答：**

```
cppCopyEdit#include <opencv2/opencv.hpp>
using namespace cv;
int main() {
    Mat img = imread("test.png", IMREAD_GRAYSCALE);
    Mat grad_x, grad_y, abs_grad_x, abs_grad_y, grad;
    Sobel(img, grad_x, CV_16S, 1, 0, 3);
    Sobel(img, grad_y, CV_16S, 0, 1, 3);
    convertScaleAbs(grad_x, abs_grad_x);
    convertScaleAbs(grad_y, abs_grad_y);
    addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0, grad);
    imshow("Sobel Edge", grad);
    waitKey(0);
}
```

------

**17. 问：Python+PyTorch 实现一个二分类 CNN。**

**答：**

```
pythonCopyEditimport torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(16*16*16, 2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
```

------

**18. 问：如何用 OpenCV 在 C++ 中实现透视变换？**

**答：**

```
cppCopyEditPoint2f src[4] = { {0,0}, {100,0}, {100,100}, {0,100} };
Point2f dst[4] = { {10,10}, {80,20}, {90,90}, {20,80} };
Mat M = getPerspectiveTransform(src, dst);
warpPerspective(img, out, M, img.size());
```

------

**19. 问：Python 实现读取视频流并显示 FPS。**

**答：**

```
pythonCopyEditimport cv2, time
cap = cv2.VideoCapture(0)
prev = time.time()
while True:
    ret, frame = cap.read()
    now = time.time()
    fps = 1 / (now - prev)
    prev = now
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
    cv2.imshow("Video", frame)
    if cv2.waitKey(1) == 27:
        break
```

------

**20. 问：Python 如何实现随机裁剪与颜色抖动数据增强？**

**答：**

```
pythonCopyEditfrom torchvision import transforms
transform = transforms.Compose([
    transforms.RandomCrop(224),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor()
])
```

------

## **四、项目经验与思考（权重 10%）**

------

**21. 问：描述你参与过的一个感知算法项目及核心贡献。**

**答**：场景背景 → 技术方案（网络结构、数据处理） → 个人贡献（算法优化/部署） → 指标提升（mAP、FPS）。

------

**22. 问：一次模型部署失败的案例及解决方法？**

**答**：问题复现 → 日志分析 → 确认原因（算子不支持/精度下降） → 修复或替换 → 回归测试。

------

**23. 问：如何提升夜间、雨雾天气下的感知鲁棒性？**

**答**：多传感器融合（红外、雷达）、低光增强、去雾算法、夜间专用数据集训练。

------

**24. 问：你是如何管理和维护大规模数据集的？**

**答**：版本控制（DVC）、标注质量审核、数据索引与分片、冷热数据分离。

------

**25. 问：为什么端到端感知+预测模型车端部署难？**

**答**：计算资源高、延迟限制、可解释性差、调试难度大。

------

**26. 问：你怎么看 BEV 的未来地位？**

**答**：融合几何+学习优势，未来结合时序建模和多模态（语言+传感器）。

------

**27. 问：车端算力限制时，你会优先压缩模型还是改结构？**

**答**：先结构优化再配合量化/剪枝，保持精度同时提升速度。

------

**28. 问：你有过算法到工程落地的闭环经验吗？**

**答**：数据 → 算法设计 → 训练 → 优化 → 部署 → 车端验证 → bug 修复。

------

**29. 问：自动驾驶感知最大挑战是什么？**

**答**：长尾场景、多传感器融合一致性、实时性与精度平衡。

------

**30. 问：你对未来 2-3 年感知算法发展趋势的预测？**

**答**：多任务端到端、多模态融合、更高效轻量化部署。



