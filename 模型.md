# ParkingE2E
**1. 输入与 BEV 表征**

输入：环视摄像头 RGB 图像 + 用户指定的目标车位。

图像特征提取：使用 EfficientNet 提取多视角图像特征。

深度预测与 BEV 转换：借鉴 LSS（Lift-Splat-Shoot） 方法，预测每个像素的深度分布，并将图像特征投影到鸟瞰图（BEV）空间。

目标车位编码：将目标车位生成 BEV 热力图，经 CNN 提取为目标特征。

**2. 特征融合（Target Query）**

利用 交叉注意力机制（cross-attention），用目标车位特征作为 Query，图像 BEV 特征作为 Key/Value，得到融合特征 Ffuse。

这样可以显式地将目标车位与环境信息对应起来。

**3. Trajectory Decoder**

将泊车轨迹序列化为离散 token（类似 NLP 中的序列建模）。

使用 Transformer Decoder 逐点自回归预测未来轨迹点（waypoints），而不是像传统方法那样使用 GRU。

相比 GRU，Transformer 具备全局感受野，预测更准确。


---

# LSS

1. Lift（图像到3D特征）

输入：多相机图像 + 相机内外参。

每个像素预测一个 深度分布 α 和一个 上下文向量 c。

在一系列离散深度位置上展开，形成三维点云特征（相机视锥 frustum 内的上下文特征）。

这一步类似于 “pseudo-LiDAR”，但网络能自动学习在何处放置信息（选择单一深度或分散到整条射线）。

2. Splat（投影融合）

将所有相机的 frustum 特征映射到统一的 BEV 网格（采用 pillar pooling 技术）。

得到固定大小的 BEV 表征张量（C×H×W），可由标准 CNN 处理。

这一步实现了不同相机视角的对齐和信息融合，保持了平移、相机顺序、坐标变换的对称性。

3. Shoot（运动规划）

使用 BEV 表征预测 代价图 cost map。

将一组离线聚类得到的 轨迹模板（1K 条）“射入”代价图并计算代价分布。

训练时使用专家轨迹的最近邻模板作为监督，推理时选择代价最低的轨迹。

这样实现了端到端的、可解释的运动规划。

---

# MBconv
先升维（扩展通道），再卷积，再降维。

**1.扩展阶段（Expand, 1×1 卷积）**

用一个 1×1 卷积把通道数从 C → t·C（t 为扩展因子，常用 6）。

相当于把特征空间升高，增加表达能力。

**2.深度卷积（Depthwise Convolution, 3×3 或 5×5）**

**对每个通道**单独做卷积（depthwise conv），计算量小。

只提取空间信息，不改变通道数。

常用 stride=1 或 2（stride=2 时同时下采样）。

**3.投影阶段（Project, 1×1 卷积）**

用另一个 1×1 卷积把通道数从 t·C → C’。

相当于把高维特征压缩回输出维度，形成“瓶颈”。

**4.跳跃连接（Skip Connection）**

如果输入和输出大小、通道一致，就加上 残差连接（类似 ResNet）。

有时结合 SE 模块（Squeeze-and-Excitation），称为 MBConv + SE。

## Bottleneck vs MBConv（倒置瓶颈）
**Bottleneck（ResNet**）：

先降维 → 3×3 卷积 → 升维

目标是 减少计算量。增加网络深度。

**Inverted Bottleneck（MBConv）：**

先升维 → depthwise 卷积 → 降维

目标是 提高特征表达能力，同时保持轻量化（因为 depthwise 卷积计算很省）。
