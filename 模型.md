# ParkingE2E
**1. 输入与 BEV 表征**

输入：环视摄像头 RGB 图像 + 用户指定的目标车位。

图像特征提取：使用 EfficientNet 提取多视角图像特征。

深度预测与 BEV 转换：借鉴 LSS（Lift-Splat-Shoot） 方法，预测每个像素的深度分布，并将图像特征投影到鸟瞰图（BEV）空间。

目标车位编码：将目标车位生成 BEV 热力图，经 CNN 提取为目标特征。

**2. 特征融合（Target Query）**

利用 交叉注意力机制（cross-attention），用目标车位特征作为 Query，图像 BEV 特征作为 Key/Value，得到融合特征 Ffuse。

这样可以显式地将目标车位与环境信息对应起来。

**3. Trajectory Decoder**

将泊车轨迹序列化为离散 token（类似 NLP 中的序列建模）。

使用 Transformer Decoder 逐点自回归预测未来轨迹点（waypoints），而不是像传统方法那样使用 GRU。

相比 GRU，Transformer 具备全局感受野，预测更准确。


---

# LSS

1. Lift（图像到3D特征）

输入：多相机图像 + 相机内外参。

每个像素预测一个 深度分布 α 和一个 上下文向量 c。

在一系列离散深度位置上展开，形成三维点云特征（相机视锥 frustum 内的上下文特征）。

这一步类似于 “pseudo-LiDAR”，但网络能自动学习在何处放置信息（选择单一深度或分散到整条射线）。

2. Splat（投影融合）

将所有相机的 frustum 特征映射到统一的 BEV 网格（采用 pillar pooling 技术）。

得到固定大小的 BEV 表征张量（C×H×W），可由标准 CNN 处理。

这一步实现了不同相机视角的对齐和信息融合，保持了平移、相机顺序、坐标变换的对称性。

3. Shoot（运动规划）

使用 BEV 表征预测 代价图 cost map。

将一组离线聚类得到的 轨迹模板（1K 条）“射入”代价图并计算代价分布。

训练时使用专家轨迹的最近邻模板作为监督，推理时选择代价最低的轨迹。

这样实现了端到端的、可解释的运动规划。

---

# MBconv
先升维（扩展通道），再卷积，再降维。

**1.扩展阶段（Expand, 1×1 卷积）**

用一个 1×1 卷积把通道数从 C → t·C（t 为扩展因子，常用 6）。

相当于把特征空间升高，增加表达能力。

**2.深度卷积（Depthwise Convolution, 3×3 或 5×5）**

**对每个通道**单独做卷积（depthwise conv），计算量小。

只提取空间信息，不改变通道数。

常用 stride=1 或 2（stride=2 时同时下采样）。

**3.投影阶段（Project, 1×1 卷积）**

用另一个 1×1 卷积把通道数从 t·C → C’。

相当于把高维特征压缩回输出维度，形成“瓶颈”。

**4.跳跃连接（Skip Connection）**

如果输入和输出大小、通道一致，就加上 残差连接（类似 ResNet）。

有时结合 SE 模块（Squeeze-and-Excitation），称为 MBConv + SE。

## Bottleneck vs MBConv（倒置瓶颈）
**Bottleneck（ResNet**）：

先降维 → 3×3 卷积 → 升维

目标是 减少计算量。增加网络深度。

**Inverted Bottleneck（MBConv）：**

先升维 → depthwise 卷积 → 降维

目标是 提高特征表达能力，同时保持轻量化（因为 depthwise 卷积计算很省）。

---

# BEVFormer
<img width="2410" height="942" alt="Screenshot from 2025-09-21 19-01-24" src="https://github.com/user-attachments/assets/a222d9de-c507-4261-9551-9be71f861c34" />

BEVFormer 主要解决两个问题：

- 如何把多相机的 2D 图像特征转化为 BEV 表示

- 如何在 BEV 表示中同时融合空间和时间信息

**为此，模型引入了三大关键设计：**

**BEV Queries：** 预定义的网格状可学习参数，每个 query 对应 BEV 平面上的一个区域。

**Spatial Cross-Attention：** 通过相机外参将 BEV query 投影到多相机图像上，只和相关区域交互，利用 deformable attention 高效采样特征。偏移点的学习是隐式的、由任务驱动的。

**Temporal Self-Attention：** 在 BEV 层面上，将当前帧和历史帧的 BEV 特征对齐（考虑自车运动），然后通过注意力机制融合历史信息，从而增强速度估计和遮挡目标检测能力。


---

# GKT

**GKT 则结合了两者优势：**

- 使用 相机参数做粗投影，得到 BEV 网格对应的近似 2D 区域。

- 在该区域上展开一个 Kh×Kw 的 kernel patch，作为候选特征区域。

- BEV query 与这些 kernel 特征通过 Transformer 注意力机制交互，从而生成 BEV 表征。

这样既利用了几何先验，又不过度依赖精确标定，提升了效率与鲁棒性。

---

# Fast-Bev
<img width="1385" height="685" alt="Screenshot from 2025-09-15 10-05-24" src="https://github.com/user-attachments/assets/e162e677-fbd6-45a4-842d-24a1b6de6fa0" />

Fast-BEV 由五大核心模块组成

**1.Fast-Ray Transformation（快速射线变换）**

将多视角 2D 图像特征直接投影到 3D 体素空间。

使用 Look-Up-Table (LUT) 预先存储投影索引，大幅减少推理时的计算。

采用 Multi-View to One-Voxel 策略，把所有相机的特征投影到同一个体素，避免稀疏体素的昂贵聚合。

相比 LSS 或 Transformer-based 方法，推理速度提升数十倍。

**2.Multi-Scale Image Encoder（多尺度图像编码器）**

使用 FPN 结构输出 1/4、1/8、1/16 三层特征。

结合 Fast-Ray 投影实现多尺度 BEV 特征，提升小目标检测和远距离感知效果。

**3.Efficient BEV Encoder（高效 BEV 编码器）**

- 引入 Space-to-Channel (S2C)

目的：减少 BEV 特征图在空间维度上的分辨率，从而降低计算量。

```
BEV 特征图 [B, C, H, W]，假设 s=2，
输出就是 [B, C×4, H/2, W/2]。
```
- Multi-Scale Concatenation Fusion (MSCF)
   
目的：充分融合不同尺度的 BEV 特征，提高检测小目标/远距离物体的能力。

   把FPN后这些不同尺度的 BEV 特征图 采样到 统一的 BEV 网格分辨率 ，直接在通道维度 concat，再经过一个卷积层融合。
   
- Multi-Frame Concatenation Fusion (MFCF)

目的：在 BEV 空间做时序融合，提高遮挡场景和速度估计能力。

对当前帧和历史帧（比如前 3 帧）的 BEV 特征，先通过 自车运动（ego-motion）补偿对齐到同一坐标系；

然后把这些 BEV 特征在通道维度 拼接 (concat)；

最后用轻量卷积融合。

---

### Pointpillars
1. 将点云分配到bev下的pillar中
```
xi = int((x - x_min) / voxel_size_x)
yi = int((y - y_min) / voxel_size_y)
zi = int((z - z_min) / voxel_size_z)
```

就能知道它属于哪个 pillar

2. 每个 pillar 最多保存 max_num_points=32 个点，多的丢弃，不足的补零。

3. 每个点的最终输入特征变为：(x,y,z,r,xc​,yc​,xp​,yp​,zp​)

- r ：反射强度 

- xc​,yc :相对 pillar 中心的偏移：(xc​,yc​)=(x−xpillar​,y−ypillar​)

- xp​,yp​,zp​ 相对 pillar 中所有点的均值(重心)的偏移：(xp​,yp​,zp​)=(x−xˉ,y−yˉ​,z−zˉ)

4. 将每个点的 9 维特征送入一个 共享的多层感知机 (MLP),得到高维特征向量，例如 64 维,相当于对每个点做“逐点特征提取”。

5.最大池化 (max pooling)，每一个pillar 得到一个 特征向量（例如 64 维）。

---
# 检测

# centernet

在bev 特征后接两个head，heatmap head是**分类头**，框体信息是**回归头**

**分类头**

代表了每个位置检测的置信度 [B, Nclass, H, W]

**回归头：** 

dx, dy → 中心点的亚像素偏移（补偿 BEV 下采样）

z → 物体中心高度

w, l, h → 物体三维尺寸

yaw → 朝向角

```
self.heatmap_head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, num_classes, kernel_size=1)
        )
self.reg_head = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, num_reg, kernel_size=1)
        )

heatmap = self.heatmap_head(x).sigmoid()  # [B, num_classes, H, W]
        reg = self.reg_head(x)

```
## 输出 top_k
```
def decode_predictions(heatmap, reg, K=100, downsample=4, voxel_size=0.2):
    # heatmap: [B, C, H, W]
    # reg: [B, 7, H, W]  (dx, dy, z, w, l, h, yaw)
    B, C, H, W = heatmap.shape
    results = []
    
    for b in range(B):
        # 1. 取 top-K
        scores, indices = torch.topk(heatmap[b].view(-1), K) #把 [C, H, W] 展平成一维向量。
        classes = indices // (H*W)    #这些点在展平后的索引
        coords = indices % (H*W)
        ys = coords // W
        xs = coords % W

        # 2. 取回归参数
        dx, dy, z, w, l, h, yaw = [reg[b,i,ys,xs] for i in range(7)]

        # 3. 转换到真实坐标
        x_real = (xs + dx) * downsample * voxel_size
        y_real = (ys + dy) * downsample * voxel_size

        boxes = torch.stack([x_real, y_real, z, w, l, h, yaw, scores, classes], dim=-1)
        results.append(boxes)

    return results  # [B, K, 9]
```

## Loss 训练时 (Focal +L1 loss)
```
def focal_loss(pred, gt, alpha=2, beta=4):
    """
    pred: [B, C, H, W] —— 模型预测，sigmoid 后
    gt:   [B, C, H, W] —— 高斯真值
    """
    pos_mask = (gt == 1).float()
    neg_mask = (gt < 1).float()

    pos_loss = - (1 - pred) ** alpha * torch.log(pred + 1e-6) * pos_mask
    neg_loss = - (pred ** alpha) * ((1 - gt) ** beta) * torch.log(1 - pred + 1e-6) * neg_mask

    num_pos = pos_mask.sum()
    loss = (pos_loss.sum() + neg_loss.sum()) / torch.clamp(num_pos, min=1.0)
    return loss

```
训练时，检测 head 的总损失一般是：

$$
L=Lheatmap​+λxy​Lxy​+λsize​Lsize​+λyaw​Lyaw​+λz​Lz​
$$

## NMS 推理时

前面我们已经得到 100 个候选框，每个框是：(x,y,z,w,l,h,yaw,score,class)

(1) 按置信度排序

先把这 100 个框按照 score 从大到小排序。

(2) NMS（非极大值抑制）

在 BEV 平面（x,y,w,l,yaw）上计算 IoU。

遍历排序后的候选框：

如果与已有保留框的 IoU < 阈值（比如 0.5），就保留；

(3) 按类别做 NMS

不同类别之间通常不会互相抑制（车 vs 行人）。

所以一般是 对每个类别单独做 NMS。

# DMPR

**1.CNN 特征提取**

输入环视图像 → 卷积网络提取特征 (S×S×N)。

每个网格单元预测一个标点。

**2.回归输出 (6 个分量)**

cx, cy：标点相对网格的偏移；

s：形状（L/T）；

cosθ, sinθ：方向角；

C：置信度。

**3.损失函数**

位置、形状、方向用 L2 损失；

置信度用二分类损失；

总损失为加权和 。


**二分类损失**

$$
L=−(ylog(p)+(1−y)log(1−p))
$$
```
probs = torch.sigmoid(logits)

# 按公式计算 BCE
loss = - (targets * torch.log(probs + 1e-6) + (1 - targets) * torch.log(1 - probs + 1e-6))
loss = loss.mean()
```
